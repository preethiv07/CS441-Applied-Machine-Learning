# Agenda
![alt text](image-11.png)

> - trining neural network
> - min batch SGD to train - to find parameters
> - apply cross vaolidation for regularization

# Simple Neuraal network
![alt text](image-12.png)
> - recap
> -Relu - for splitting
> - softwmnax- balance prob for units
> - we get piecewise linear

# Cost
![alt text](image-13.png)

> - training classifier - needs to find vector of paramerters - theta
> - theta - vector of parameters of unit "j" (weight and bias)
> - cost should achieve 2 goal
> - max log likelihoood (Loss term)
> - minimize parameters (penalty term)
> - LOSS TERM (Log Loss,Cross Entropy loss)
> - right SIDE
> - each unit - we want to min. negative log - under prob 
> -  Avg. loss of negative log
> - normalising by size of dataset will ensure the results dont depend on data set
> - one hot vector (y) - used as indicator vector
> - PENALIZATION
> - minimize size of parametrs - so we penalize large weigths
> - sum of L2 norms 
> - Cost = Loss +Penalization term


# training
![alt text](image-14.png)
> - SGD to train the model to minimize the Cost method
> - search parameters for low cost
> - CONS: cannot find global minimum
> - Low loss is good enough
> - MINI BATCHES - subset of data
> - size of min batches (M) > 1 and smaller than actual dataset
> - M is chosen by architecture and memory layer processing
> - Training:
> - Iterate for predefined number of mini batches
> - mini batch frommrandom uniform dist.
> - apply SGD and search continues
> - we develop gradient for cost function
> 

# Gradient -1
![alt text](image-15.png)
> - cost = loss +penalty
> -  Penalty is L2 metrics (
> - gradient - two partial derivation
> - 1. w.r.t weights
> - 2. w.r.t bias (its euqated to zero)
> 

# Gradient -2
![alt text](image-16.png)

> - Loss term for data item "i"
> - one class per unit
> - solving derivative for eaach unit
> - ignore summation and consider loss term for one unit "u"
> gradient - two terms - weights and bias
> use chain rule to combine two derivative , each for weight and bias
> - one common term on the derivative on weight and bias

>- RIGHT SIDE
> - Indicator function
> - work on each partial derivative
> - u=v, (it is 1-s)
> - u not qual to v, outout is 0-s) 

# Gradient -3
![alt text](image-17.png)

> - if o is positive, output corresponding to input , derivative = x(a)
> - Expression definition

> When the unit u corresponds to class j 
> > - the output depends on o_v, 
> > - which can be either positive or 0. 

> If o_v is positive, 
> > - then it means that the output corresponds to the input x_a, 
> > - weighted by w_a and the derivative is then x_a. 

> If o_v is 0, 
> > - then the derivative is 0, 
> > - and when the unit u does not correspond to class j, the derivative should be 0.

- same applies for bias

# Training
![alt text](image-18.png)

> - triaining is iterative process
> 1. select minibatch "M"
> 2. apply SGD in each minibatch
> 3. SGD is also iterative process
> 4. Gradient descent step "n", compute for parameter "N+1"
> 5. step size is EEta(N)
> 6. Theta = parameter for next step is derived from previous step and step size(eeta)
> 7. step size = learning rate
> 8. longer steps in early iteration and smaller for later iteration
> 9. learning into epoch
> - Epoch size - through expierimation
> - gamma - positive constant (thrugh experiment)
> RIGHT SIDE
> - no clear stopping criteria
> - training continues until user defined number of epochs
> - proper value of epoch number is through experiements
> - regularization term (lambda) - choose through cross validation
> - we use held out and trianing set.
> - evalaute training set against held out set for error
> - We can use accuracy or error rate to identify best performance
> - then train using whole dataset


# Recap
![alt text](image-19.png)

---
# The end

