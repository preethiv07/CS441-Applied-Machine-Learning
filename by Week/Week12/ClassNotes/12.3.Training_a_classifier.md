# Agenda
![alt text](image-11.png)

> - trining neural network
> - min batch SGD to train - to find parameters
> - apply cross vaolidation for regularization

# Simple Neuraal network
![alt text](image-12.png)
> - recap
> -Relu - for splitting
> - softwmnax- balance prob for units
> - we get piecewise linear

# Cost
![alt text](image-13.png)

> - training classifier - needs to find vector of paramerters - theta
> - theta - vector of parameters of unit "j" (weight and bias)
> - cost should achieve 2 goal
> - max log likelihoood (Loss term)
> - minimize parameters (penalty term)
> - LOSS TERM (Log Loss,Cross Entropy loss)
> - right SIDE
> - each unit - we want to min. negative log - under prob 
> -  Avg. loss of negative log
> - normalising by size of dataset will ensure the results dont depend on data set
> - one hot vector (y) - used as indicator vector
> - PENALIZATION
> - minimize size of parametrs - so we penalize large weigths

# training
![alt text](image-14.png)

# Gradient -1
![alt text](image-15.png)

# Gradient -2
![alt text](image-16.png)

# Gradient -3
![alt text](image-17.png)

# Training
![alt text](image-18.png)

# Recap
![alt text](image-19.png)


